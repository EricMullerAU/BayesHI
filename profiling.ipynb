{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42dc982d-c846-402f-ad18-2cf6dc6db0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import bayeshi\n",
    "\n",
    "from pyinstrument import Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be504334-be88-43c5-83f4-ae5e87787739",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e73c6-4c71-41dc-b901-80fed8ea9cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9c030-5900-4387-b647-f1f812f92568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c639b2-be35-4c95-a414-5e262358838c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd815d58-4e59-4898-b8c1-672fcc9fad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TIGRESS simulation cube 290 (1/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 300 (2/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 310 (3/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 320 (4/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 330 (5/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 340 (6/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 350 (7/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 360 (8/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 370 (9/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 380 (10/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 390 (11/11) with x_values=emission and y_values=fractions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 2.60735 s\n",
       "File: /home/120/em8117/BayesHI/bayeshi/data_loaders.py\n",
       "Function: load_tigress_data at line 275\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   275                                           def load_tigress_data(data_path, sim_number='all', x_values='emission', y_values='fractions', verbose=False):\n",
       "   276         1       2546.0   2546.0      0.0      if sim_number == 'all':\n",
       "   277         1      23311.0  23311.0      0.0          sim_number = np.arange(290, 391, 10)\n",
       "   278                                               elif type(sim_number) is list:\n",
       "   279                                                   sim_number = [int(s) for s in sim_number]\n",
       "   280                                               else:\n",
       "   281                                                   try:\n",
       "   282                                                       sim_number = [int(sim_number)]\n",
       "   283                                                   except:\n",
       "   284                                                       raise ValueError(\"sim_number should be 'all', a list of integers, or a single integer.\")\n",
       "   285                                                   \n",
       "   286         1       1240.0   1240.0      0.0      if x_values not in ['emission', 'absorption']:\n",
       "   287                                                   raise ValueError(\"x_values must be either 'emission' or 'absorption'\")\n",
       "   288         1        701.0    701.0      0.0      if y_values not in ['fractions', 'absorption', 'emission']:\n",
       "   289                                                   raise ValueError(\"y_values must be either 'fractions', 'absorption', or 'emission'\")\n",
       "   290                                           \n",
       "   291                                               # x_data = np.array([])\n",
       "   292                                               # y_data = np.array([])\n",
       "   293                                               # Preallocate arrays to hold data\n",
       "   294                                               # All TIGRESS cubes are 256 x 256 x 512 (v, z, x)\n",
       "   295         1       2433.0   2433.0      0.0      total_spectra = len(sim_number) * 256 * 512\n",
       "   296         1      32831.0  32831.0      0.0      x_data = np.empty((total_spectra, 256))  # 256 channels\n",
       "   297         1       1255.0   1255.0      0.0      if y_values == 'fractions':\n",
       "   298         1      18284.0  18284.0      0.0          y_data = np.empty((total_spectra, 4))\n",
       "   299                                               else:\n",
       "   300                                                   y_data = np.empty((total_spectra, 256))\n",
       "   301                                               \n",
       "   302        12      57222.0   4768.5      0.0      for i, sim in enumerate(sim_number):\n",
       "   303        11       8811.0    801.0      0.0          if verbose:\n",
       "   304        11    1124157.0 102196.1      0.0              print(f'Loading TIGRESS simulation cube {sim} ({i+1}/{len(sim_number)}) with x_values={x_values} and y_values={y_values}')\n",
       "   305        11       6263.0    569.4      0.0          if x_values == 'emission':\n",
       "   306        11  119626079.0    1e+07      4.6              spectra = fits.getdata(data_path + f'{sim}_Tb_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   307                                                   elif x_values == 'absorption':\n",
       "   308                                                       spectra = fits.getdata(data_path + f'{sim}_Tau_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   309                                                   else:\n",
       "   310                                                       raise ValueError(\"x_values must be either 'emission' or 'absorption'\")\n",
       "   311                                                           \n",
       "   312                                                   # Change from (v, z, x) to (x*z, v)\n",
       "   313        11     454693.0  41335.7      0.0          spectra = np.moveaxis(spectra, 0, -1)\n",
       "   314        11      41468.0   3769.8      0.0          spectra = spectra.reshape(-1, spectra.shape[-1])\n",
       "   315                                                   \n",
       "   316                                                   # Add the spectra to x_data\n",
       "   317        11       9496.0    863.3      0.0          start_idx = (i * 256 * 512)\n",
       "   318        11      10053.0    913.9      0.0          end_idx = start_idx + spectra.shape[0]\n",
       "   319        11 2288546734.0    2e+08     87.8          x_data[start_idx:end_idx, :] = spectra\n",
       "   320                                                   \n",
       "   321        11      39740.0   3612.7      0.0          if y_values == 'fractions':\n",
       "   322        11   46872011.0    4e+06      1.8              fcnm = fits.getdata(data_path + f'{sim}_fcnm_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   323        11   43309489.0    4e+06      1.7              funm = fits.getdata(data_path + f'{sim}_funm_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   324        11   38092210.0    3e+06      1.5              fwnm = fits.getdata(data_path + f'{sim}_fwnm_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   325        11   35660542.0    3e+06      1.4              rhi = fits.getdata(data_path + f'{sim}_rhi_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   326                                                       \n",
       "   327                                                       # Stack from (z, x) to (z, x, 4)\n",
       "   328        11   20989272.0    2e+06      0.8              fractions = np.stack((fcnm, funm, fwnm, rhi), axis=2)\n",
       "   329                                                       # Change from (z, x, 4) to (x*z, 4)\n",
       "   330        11      33887.0   3080.6      0.0              fractions = fractions.reshape(-1, 4)\n",
       "   331        11      16311.0   1482.8      0.0              y_data_temp = fractions\n",
       "   332                                                       \n",
       "   333                                                   elif y_values == 'absorption':\n",
       "   334                                                       absorption = fits.getdata(data_path + f'{sim}_Tau_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   335                                                       # Change from (v, z, x) to (x*z, v)\n",
       "   336                                                       absorption = np.moveaxis(absorption, 0, -1)\n",
       "   337                                                       y_data_temp = absorption.reshape(-1, absorption.shape[-1])\n",
       "   338                                                   elif y_values == 'emission':\n",
       "   339                                                       emission = fits.getdata(data_path + f'{sim}_Tb_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   340                                                       # Change from (v, z, x) to (x*z, v)\n",
       "   341                                                       emission = np.moveaxis(emission, 0, -1)\n",
       "   342                                                       y_data_temp = emission.reshape(-1, emission.shape[-1])\n",
       "   343                                                   else:\n",
       "   344                                                       raise ValueError(\"y_values must be either 'fractions', 'absorption', or 'emission'\")\n",
       "   345                                               \n",
       "   346                                                   # Add the y_data to y_data\n",
       "   347        11   12366146.0    1e+06      0.5          y_data[start_idx:end_idx, :] = y_data_temp\n",
       "   348                                                       \n",
       "   349         1       4070.0   4070.0      0.0      return x_data, y_data"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f bayeshi.data_loaders.load_tigress_data -s bayeshi.data_loaders.load_tigress_data(data_path='/scratch/mk27/em8117/TIGRESS/', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11e350-cfed-4ae9-a0ff-55c8d45407d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29113e17-e5c4-43d8-9a2e-272941f33b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TIGRESS simulation cube 290 (1/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 300 (2/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 310 (3/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 320 (4/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 330 (5/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 340 (6/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 350 (7/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 360 (8/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 370 (9/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 380 (10/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 390 (11/11) with x_values=emission and y_values=fractions\n",
      "Total number of spectra: 1441792\n",
      "Splitting data into 60% train, 20% validation, and 20% test sets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 17.9356 s\n",
       "File: /home/120/em8117/BayesHI/bayeshi/data_loaders.py\n",
       "Function: load_data at line 66\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    66                                           def load_data(data_path='/scratch/mk27/em8117/', x_values='emission', y_values='fractions',\n",
       "    67                                                         dataset='all', tigress_sim='all', seta_sim='both', split=None,\n",
       "    68                                                         batch_size=32, num_workers=4, noise=0.5, test_size=0.2, val_size=0.2,\n",
       "    69                                                         random_state=42, show_example=False, verbose=False):\n",
       "    70                                               # If you need to save space in memory and are loading a subset of the data, I recommend not using all the TIGRESS cubes.\n",
       "    71                                               # Instead, select only a few of them as loading all 11 cubes just to subsample at the end is not efficient.\n",
       "    72                                           \n",
       "    73                                               # Validate x_values and y_values\n",
       "    74         1        744.0    744.0      0.0      if x_values not in ['emission', 'absorption']:\n",
       "    75                                                   raise ValueError(\"x_values must be either 'emission' or 'absorption'\")\n",
       "    76         1        227.0    227.0      0.0      if y_values not in ['fractions', 'absorption', 'emission']:\n",
       "    77                                                   raise ValueError(\"y_values must be either 'fractions', 'absorption', or 'emission'\")\n",
       "    78                                           \n",
       "    79                                               # Create empty arrays to be overwritten as needed\n",
       "    80         1       8026.0   8026.0      0.0      tigress_x, tigress_y = np.array([]), np.array([])\n",
       "    81         1        957.0    957.0      0.0      saury_x, saury_y = np.array([]), np.array([])\n",
       "    82         1        818.0    818.0      0.0      seta_x, seta_y = np.array([]), np.array([])\n",
       "    83                                           \n",
       "    84                                               # Load data based on dataset parameter\n",
       "    85         1        473.0    473.0      0.0      if dataset == 'all':\n",
       "    86                                                   tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "    87                                                   saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "    88                                                   seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "    89         1        949.0    949.0      0.0      elif isinstance(dataset, list):\n",
       "    90                                                   for sim in dataset:\n",
       "    91                                                       if sim.startswith('tigress'):\n",
       "    92                                                           tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "    93                                                       elif sim.startswith('saury'):\n",
       "    94                                                           saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "    95                                                       elif sim.startswith('seta'):\n",
       "    96                                                           seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "    97                                                       else:\n",
       "    98                                                           raise ValueError(f\"Unknown dataset type: {sim}\")\n",
       "    99         1        579.0    579.0      0.0      elif isinstance(dataset, str):\n",
       "   100         1        750.0    750.0      0.0          if dataset.startswith('tigress'):\n",
       "   101         1 2607682876.0    3e+09     14.5              tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "   102                                                   elif dataset.startswith('saury'):\n",
       "   103                                                       saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "   104                                                   elif dataset.startswith('seta'):\n",
       "   105                                                       seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "   106                                                   else:\n",
       "   107                                                       raise ValueError(f\"Unknown dataset type: {dataset}\")\n",
       "   108                                               else:\n",
       "   109                                                   raise ValueError(\"dataset must be 'all', a list of dataset names, or a single dataset name string\")\n",
       "   110                                           \n",
       "   111                                               # Collect non-empty simulations\n",
       "   112         1       2264.0   2264.0      0.0      sim_data = {\n",
       "   113         1      11819.0  11819.0      0.0          'tigress': (tigress_x, tigress_y) if tigress_x.size > 0 else None,\n",
       "   114         1       1651.0   1651.0      0.0          'saury': (saury_x, saury_y) if saury_x.size > 0 else None,\n",
       "   115         1       3831.0   3831.0      0.0          'seta': (seta_x, seta_y) if seta_x.size > 0 else None\n",
       "   116                                               }\n",
       "   117         4      12360.0   3090.0      0.0      active_sims = [k for k, v in sim_data.items() if v is not None]\n",
       "   118         1       1153.0   1153.0      0.0      if not active_sims:\n",
       "   119                                                   raise ValueError(\"No valid simulations loaded\")\n",
       "   120                                           \n",
       "   121                                               # Process split argument\n",
       "   122         1       1660.0   1660.0      0.0      if split == 'all':\n",
       "   123                                                   # Use all available data\n",
       "   124                                                   pass\n",
       "   125         1       1061.0   1061.0      0.0      elif split == 'equal':\n",
       "   126                                                   # Use equal number of samples from each loaded simulation\n",
       "   127                                                   n_per_sim = min(v[0].shape[0] for v in sim_data.values() if v is not None)\n",
       "   128                                                   rng = np.random.RandomState(random_state)\n",
       "   129                                                   for sim in active_sims:\n",
       "   130                                                       x, y = sim_data[sim]\n",
       "   131                                                       idx = rng.permutation(x.shape[0])[:n_per_sim]\n",
       "   132                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   133         1       3713.0   3713.0      0.0      elif isinstance(split, int):\n",
       "   134                                                   # Split as equally as possible across simulations\n",
       "   135                                                   n_total = split\n",
       "   136                                                   n_sims = len(active_sims)\n",
       "   137                                                   n_per_sim = n_total // n_sims\n",
       "   138                                                   remainder = n_total % n_sims\n",
       "   139                                                   print(f\"Splitting {n_total} samples across {n_sims} simulations: {n_per_sim} per simulation, with {remainder} extra samples distributed\")\n",
       "   140                                                   rng = np.random.RandomState(random_state)\n",
       "   141                                                   for i, sim in enumerate(active_sims):\n",
       "   142                                                       x, y = sim_data[sim]\n",
       "   143                                                       n = n_per_sim + (1 if i < remainder else 0)\n",
       "   144                                                       if n > x.shape[0]:\n",
       "   145                                                           raise ValueError(f\"Not enough samples in {sim} for split={split}\")\n",
       "   146                                                       idx = rng.permutation(x.shape[0])[:n]\n",
       "   147                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   148         1       1800.0   1800.0      0.0      elif isinstance(split, dict):\n",
       "   149                                                   # Dictionary split (proportion, percent, or absolute)\n",
       "   150                                                   required_keys = {'tigress', 'saury', 'seta'}\n",
       "   151                                                   if not required_keys.issubset(split.keys()):\n",
       "   152                                                       raise ValueError(\"split dictionary must contain 'tigress', 'saury', and 'seta' keys\")\n",
       "   153                                                   for key in required_keys:\n",
       "   154                                                       if not isinstance(split[key], (int, float)):\n",
       "   155                                                           raise ValueError(f\"split['{key}'] must be a number\")\n",
       "   156                                                   sum_split = sum(split.values())\n",
       "   157                                                   # Check if numbers are proportions, percentages, or absolute numbers\n",
       "   158                                                   if all(0 <= v <= 1 for v in split.values()):\n",
       "   159                                                       if np.isclose(sum_split, 1.0):\n",
       "   160                                                           is_proportion = True  # proportions\n",
       "   161                                                           # Print the splits as percentages for each simulation\n",
       "   162                                                           print(\"Splitting data as proportions:\")\n",
       "   163                                                           for sim in active_sims:\n",
       "   164                                                               print(f\"{sim}: {split[sim] * 100:.2f}%\")\n",
       "   165                                                       else:\n",
       "   166                                                           raise ValueError(\"if using proportions for data splitting, the sum of the split values must be 1.0\")\n",
       "   167                                                   elif all(0 <= v <= 100 for v in split.values()):\n",
       "   168                                                       if np.isclose(sum_split, 100.0):\n",
       "   169                                                           is_proportion = False  # percentages\n",
       "   170                                                           # Print the splits as percentages for each simulation\n",
       "   171                                                           print(\"Splitting data as percentages:\")\n",
       "   172                                                           for sim in active_sims:\n",
       "   173                                                               print(f\"{sim}: {split[sim]:.2f}%\")\n",
       "   174                                                       else:\n",
       "   175                                                           # Treat as absolute numbers if not close to 100\n",
       "   176                                                           warnings.warn(\"Splitting data as absolute numbers:\", UserWarning)\n",
       "   177                                                           for sim in active_sims:\n",
       "   178                                                               print(f\"{sim}: {split[sim]} spectra\")\n",
       "   179                                                           is_proportion = None  # absolute numbers\n",
       "   180                                                   elif all(isinstance(v, int) for v in split.values()):\n",
       "   181                                                       print(\"Splitting data as absolute numbers:\")\n",
       "   182                                                       for sim in active_sims:\n",
       "   183                                                           print(f\"{sim}: {split[sim]} spectra\")\n",
       "   184                                                       is_proportion = None  # absolute numbers\n",
       "   185                                                       if any(v < 0 for v in split.values()):\n",
       "   186                                                           raise ValueError(\"if using absolute numbers for data splitting, all values must be non-negative\")\n",
       "   187                                                   else:\n",
       "   188                                                       raise ValueError(\"split values must be either proportions (0-1), percentages (0-100), or absolute numbers (non-negative integers)\")\n",
       "   189                                                   rng = np.random.RandomState(random_state)\n",
       "   190                                                   for sim in active_sims:\n",
       "   191                                                       x, y = sim_data[sim]\n",
       "   192                                                       if is_proportion is not None:\n",
       "   193                                                           if is_proportion:\n",
       "   194                                                               n = int(round(split[sim] * x.shape[0]))\n",
       "   195                                                           else:\n",
       "   196                                                               n = int(round(split[sim] / 100.0 * x.shape[0]))\n",
       "   197                                                       else:\n",
       "   198                                                           n = min(split[sim], x.shape[0])\n",
       "   199                                                       idx = rng.permutation(x.shape[0])[:n]\n",
       "   200                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   201         1       1320.0   1320.0      0.0      elif split is not None:\n",
       "   202                                                   raise ValueError(\"split must be 'all', 'equal', an integer, a dictionary, or None\")\n",
       "   203                                           \n",
       "   204                                               # Concatenate the data\n",
       "   205         4       8807.0   2201.8      0.0      x_arrays = [v[0] for v in sim_data.values() if v is not None]\n",
       "   206         4       6101.0   1525.2      0.0      y_arrays = [v[1] for v in sim_data.values() if v is not None]\n",
       "   207         1  924484906.0    9e+08      5.2      x_data = np.concatenate(x_arrays, axis=0) if x_arrays else np.array([])\n",
       "   208         1   15055560.0    2e+07      0.1      y_data = np.concatenate(y_arrays, axis=0) if y_arrays else np.array([])\n",
       "   209                                           \n",
       "   210                                               # Concatenate the data\n",
       "   211         4       9918.0   2479.5      0.0      x_arrays = [v[0] for v in sim_data.values() if v is not None]\n",
       "   212         4       2532.0    633.0      0.0      y_arrays = [v[1] for v in sim_data.values() if v is not None]\n",
       "   213         1  925835175.0    9e+08      5.2      x_data = np.concatenate(x_arrays, axis=0) if x_arrays else np.array([])\n",
       "   214         1   15375305.0    2e+07      0.1      y_data = np.concatenate(y_arrays, axis=0) if y_arrays else np.array([])\n",
       "   215                                               \n",
       "   216                                                 \n",
       "   217         1        1e+10    1e+10     62.9      x_data += np.random.randn(*x_data.shape) * noise\n",
       "   218                                               \n",
       "   219         1     142511.0 142511.0      0.0      print('Total number of spectra:', x_data.shape[0])\n",
       "   220                                           \n",
       "   221                                               # Remove lines of sight with NaNs or all zeros\n",
       "   222                                               # nanIndices = np.isnan(spectra).any(axis=1) | np.isnan(fcnm) | np.isnan(funm) | np.isnan(fwnm) | np.isnan(rhi) | np.all(spectra == 0, axis=1)\n",
       "   223                                               # spectra = spectra[~nanIndices]\n",
       "   224                                               # fcnm = fcnm[~nanIndices]\n",
       "   225                                               # funm = funm[~nanIndices]\n",
       "   226                                               # fwnm = fwnm[~nanIndices]\n",
       "   227                                               # rhi = rhi[~nanIndices]\n",
       "   228                                               # los_removed = np.sum(nanIndices)\n",
       "   229                                                   \n",
       "   230                                               # print(f'Removed {los_removed} lines of sight with NaNs')\n",
       "   231                                               \n",
       "   232                                               # Split the data into training, validation, and testing sets\n",
       "   233         1       6896.0   6896.0      0.0      train_pct = round((1 - test_size - val_size) * 100)\n",
       "   234         1        583.0    583.0      0.0      val_pct = round(val_size * 100)\n",
       "   235         1        367.0    367.0      0.0      test_pct = round(test_size * 100)\n",
       "   236                                               \n",
       "   237         1       1568.0   1568.0      0.0      n_train_samples = round(x_data.shape[0] * (1 - test_size - val_size))\n",
       "   238         1        722.0    722.0      0.0      n_val_samples = round(x_data.shape[0] * val_size)\n",
       "   239         1        750.0    750.0      0.0      n_test_samples = round(x_data.shape[0] * test_size)\n",
       "   240                                               \n",
       "   241         1       1234.0   1234.0      0.0      if n_train_samples < 1 or n_val_samples < 1 or n_test_samples < 1:\n",
       "   242                                                   raise ValueError(\"The defined splits result in one or more datasets with less than one sample.\")\n",
       "   243                                               \n",
       "   244         1      18426.0  18426.0      0.0      print(f'Splitting data into {train_pct}% train, {val_pct}% validation, and {test_pct}% test sets.')\n",
       "   245         1 1036800783.0    1e+09      5.8      X_train, X_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size=n_val_samples+n_test_samples, random_state=random_state)\n",
       "   246         1  410862477.0    4e+08      2.3      X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=n_test_samples / (n_val_samples+n_test_samples), random_state=random_state)\n",
       "   247                                               \n",
       "   248         1  427122432.0    4e+08      2.4      train_dataset = TensorDataset(Tensor(X_train), Tensor(y_train))\n",
       "   249         1  144140115.0    1e+08      0.8      val_dataset = TensorDataset(Tensor(X_val), Tensor(y_val))\n",
       "   250         1  146937614.0    1e+08      0.8      test_dataset = TensorDataset(Tensor(X_test), Tensor(y_test))\n",
       "   251                                               \n",
       "   252         1     429719.0 429719.0      0.0      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
       "   253         1      56597.0  56597.0      0.0      val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
       "   254         1      44724.0  44724.0      0.0      test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
       "   255                                               \n",
       "   256         1        676.0    676.0      0.0      if show_example:\n",
       "   257                                                   import matplotlib.pyplot as plt\n",
       "   258                                                   random_indices = np.random.choice(spectra.shape[0], size=5, replace=False)\n",
       "   259                                                   plt.figure(figsize=(10, 6))\n",
       "   260                                                   for i in random_indices:\n",
       "   261                                                       if x_values == 'emission':\n",
       "   262                                                           plt.plot(spectra[i].T)\n",
       "   263                                                       elif x_values == 'absorption':\n",
       "   264                                                           plt.plot(np.exp(-spectra[i]).T)\n",
       "   265                                                   plt.xlabel('Channel')\n",
       "   266                                                   if x_values == 'emission':\n",
       "   267                                                       plt.ylabel(r'$T_B$ [K]')\n",
       "   268                                                   elif x_values == 'absorption':\n",
       "   269                                                       plt.ylabel(r'$e^{-\\tau}$')\n",
       "   270                                                   plt.title('Example Spectra')\n",
       "   271                                                   plt.show()\n",
       "   272                                               \n",
       "   273         1       9381.0   9381.0      0.0      return train_loader, val_loader, test_loader"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f bayeshi.load_data -s bayeshi.load_data(dataset='tigress', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345acb9-7bc2-46ea-9872-55fcc285bb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820daec8-5317-46bc-b9db-96fc43eab3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TIGRESS simulation cube 290 (1/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 300 (2/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 310 (3/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 320 (4/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 330 (5/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 340 (6/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 350 (7/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 360 (8/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 370 (9/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 380 (10/11) with x_values=emission and y_values=fractions\n",
      "Loading TIGRESS simulation cube 390 (11/11) with x_values=emission and y_values=fractions\n",
      "Loading Saury data with x_values=emission and y_values=fractions\n",
      "Loading Seta simulation comp with x_values=emission and y_values=fractions\n",
      "Total number of spectra: 2752512\n",
      "Splitting data into 60% train, 20% validation, and 20% test sets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 44.0976 s\n",
       "File: /home/120/em8117/BayesHI/bayeshi/data_loaders.py\n",
       "Function: load_data at line 66\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    66                                           def load_data(data_path='/scratch/mk27/em8117/', x_values='emission', y_values='fractions',\n",
       "    67                                                         dataset='all', tigress_sim='all', seta_sim='both', split=None,\n",
       "    68                                                         batch_size=32, num_workers=4, noise=0.5, test_size=0.2, val_size=0.2,\n",
       "    69                                                         random_state=42, show_example=False, verbose=False):\n",
       "    70                                               # If you need to save space in memory and are loading a subset of the data, I recommend not using all the TIGRESS cubes.\n",
       "    71                                               # Instead, select only a few of them as loading all 11 cubes just to subsample at the end is not efficient.\n",
       "    72                                           \n",
       "    73                                               # Validate x_values and y_values\n",
       "    74         1       1174.0   1174.0      0.0      if x_values not in ['emission', 'absorption']:\n",
       "    75                                                   raise ValueError(\"x_values must be either 'emission' or 'absorption'\")\n",
       "    76         1        452.0    452.0      0.0      if y_values not in ['fractions', 'absorption', 'emission']:\n",
       "    77                                                   raise ValueError(\"y_values must be either 'fractions', 'absorption', or 'emission'\")\n",
       "    78                                           \n",
       "    79                                               # Create empty arrays to be overwritten as needed\n",
       "    80         1      11391.0  11391.0      0.0      tigress_x, tigress_y = np.array([]), np.array([])\n",
       "    81         1       1184.0   1184.0      0.0      saury_x, saury_y = np.array([]), np.array([])\n",
       "    82         1       1237.0   1237.0      0.0      seta_x, seta_y = np.array([]), np.array([])\n",
       "    83                                           \n",
       "    84                                               # Load data based on dataset parameter\n",
       "    85         1        782.0    782.0      0.0      if dataset == 'all':\n",
       "    86         1 5697689934.0    6e+09     12.9          tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "    87         1   64500973.0    6e+07      0.1          saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "    88         1   18991821.0    2e+07      0.0          seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "    89                                               elif isinstance(dataset, list):\n",
       "    90                                                   for sim in dataset:\n",
       "    91                                                       if sim.startswith('tigress'):\n",
       "    92                                                           tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "    93                                                       elif sim.startswith('saury'):\n",
       "    94                                                           saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "    95                                                       elif sim.startswith('seta'):\n",
       "    96                                                           seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "    97                                                       else:\n",
       "    98                                                           raise ValueError(f\"Unknown dataset type: {sim}\")\n",
       "    99                                               elif isinstance(dataset, str):\n",
       "   100                                                   if dataset.startswith('tigress'):\n",
       "   101                                                       tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "   102                                                   elif dataset.startswith('saury'):\n",
       "   103                                                       saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "   104                                                   elif dataset.startswith('seta'):\n",
       "   105                                                       seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "   106                                                   else:\n",
       "   107                                                       raise ValueError(f\"Unknown dataset type: {dataset}\")\n",
       "   108                                               else:\n",
       "   109                                                   raise ValueError(\"dataset must be 'all', a list of dataset names, or a single dataset name string\")\n",
       "   110                                           \n",
       "   111                                               # Collect non-empty simulations\n",
       "   112         1       2450.0   2450.0      0.0      sim_data = {\n",
       "   113         1       4371.0   4371.0      0.0          'tigress': (tigress_x, tigress_y) if tigress_x.size > 0 else None,\n",
       "   114         1       1808.0   1808.0      0.0          'saury': (saury_x, saury_y) if saury_x.size > 0 else None,\n",
       "   115         1       1652.0   1652.0      0.0          'seta': (seta_x, seta_y) if seta_x.size > 0 else None\n",
       "   116                                               }\n",
       "   117         4      11244.0   2811.0      0.0      active_sims = [k for k, v in sim_data.items() if v is not None]\n",
       "   118         1       1233.0   1233.0      0.0      if not active_sims:\n",
       "   119                                                   raise ValueError(\"No valid simulations loaded\")\n",
       "   120                                           \n",
       "   121                                               # Process split argument\n",
       "   122         1       2584.0   2584.0      0.0      if split == 'all':\n",
       "   123                                                   # Use all available data\n",
       "   124                                                   pass\n",
       "   125         1       1096.0   1096.0      0.0      elif split == 'equal':\n",
       "   126                                                   # Use equal number of samples from each loaded simulation\n",
       "   127                                                   n_per_sim = min(v[0].shape[0] for v in sim_data.values() if v is not None)\n",
       "   128                                                   rng = np.random.RandomState(random_state)\n",
       "   129                                                   for sim in active_sims:\n",
       "   130                                                       x, y = sim_data[sim]\n",
       "   131                                                       idx = rng.permutation(x.shape[0])[:n_per_sim]\n",
       "   132                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   133         1       3744.0   3744.0      0.0      elif isinstance(split, int):\n",
       "   134                                                   # Split as equally as possible across simulations\n",
       "   135                                                   n_total = split\n",
       "   136                                                   n_sims = len(active_sims)\n",
       "   137                                                   n_per_sim = n_total // n_sims\n",
       "   138                                                   remainder = n_total % n_sims\n",
       "   139                                                   print(f\"Splitting {n_total} samples across {n_sims} simulations: {n_per_sim} per simulation, with {remainder} extra samples distributed\")\n",
       "   140                                                   rng = np.random.RandomState(random_state)\n",
       "   141                                                   for i, sim in enumerate(active_sims):\n",
       "   142                                                       x, y = sim_data[sim]\n",
       "   143                                                       n = n_per_sim + (1 if i < remainder else 0)\n",
       "   144                                                       if n > x.shape[0]:\n",
       "   145                                                           raise ValueError(f\"Not enough samples in {sim} for split={split}\")\n",
       "   146                                                       idx = rng.permutation(x.shape[0])[:n]\n",
       "   147                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   148         1       1526.0   1526.0      0.0      elif isinstance(split, dict):\n",
       "   149                                                   # Dictionary split (proportion, percent, or absolute)\n",
       "   150                                                   required_keys = {'tigress', 'saury', 'seta'}\n",
       "   151                                                   if not required_keys.issubset(split.keys()):\n",
       "   152                                                       raise ValueError(\"split dictionary must contain 'tigress', 'saury', and 'seta' keys\")\n",
       "   153                                                   for key in required_keys:\n",
       "   154                                                       if not isinstance(split[key], (int, float)):\n",
       "   155                                                           raise ValueError(f\"split['{key}'] must be a number\")\n",
       "   156                                                   sum_split = sum(split.values())\n",
       "   157                                                   # Check if numbers are proportions, percentages, or absolute numbers\n",
       "   158                                                   if all(0 <= v <= 1 for v in split.values()):\n",
       "   159                                                       if np.isclose(sum_split, 1.0):\n",
       "   160                                                           is_proportion = True  # proportions\n",
       "   161                                                           # Print the splits as percentages for each simulation\n",
       "   162                                                           print(\"Splitting data as proportions:\")\n",
       "   163                                                           for sim in active_sims:\n",
       "   164                                                               print(f\"{sim}: {split[sim] * 100:.2f}%\")\n",
       "   165                                                       else:\n",
       "   166                                                           raise ValueError(\"if using proportions for data splitting, the sum of the split values must be 1.0\")\n",
       "   167                                                   elif all(0 <= v <= 100 for v in split.values()):\n",
       "   168                                                       if np.isclose(sum_split, 100.0):\n",
       "   169                                                           is_proportion = False  # percentages\n",
       "   170                                                           # Print the splits as percentages for each simulation\n",
       "   171                                                           print(\"Splitting data as percentages:\")\n",
       "   172                                                           for sim in active_sims:\n",
       "   173                                                               print(f\"{sim}: {split[sim]:.2f}%\")\n",
       "   174                                                       else:\n",
       "   175                                                           # Treat as absolute numbers if not close to 100\n",
       "   176                                                           warnings.warn(\"Splitting data as absolute numbers:\", UserWarning)\n",
       "   177                                                           for sim in active_sims:\n",
       "   178                                                               print(f\"{sim}: {split[sim]} spectra\")\n",
       "   179                                                           is_proportion = None  # absolute numbers\n",
       "   180                                                   elif all(isinstance(v, int) for v in split.values()):\n",
       "   181                                                       print(\"Splitting data as absolute numbers:\")\n",
       "   182                                                       for sim in active_sims:\n",
       "   183                                                           print(f\"{sim}: {split[sim]} spectra\")\n",
       "   184                                                       is_proportion = None  # absolute numbers\n",
       "   185                                                       if any(v < 0 for v in split.values()):\n",
       "   186                                                           raise ValueError(\"if using absolute numbers for data splitting, all values must be non-negative\")\n",
       "   187                                                   else:\n",
       "   188                                                       raise ValueError(\"split values must be either proportions (0-1), percentages (0-100), or absolute numbers (non-negative integers)\")\n",
       "   189                                                   rng = np.random.RandomState(random_state)\n",
       "   190                                                   for sim in active_sims:\n",
       "   191                                                       x, y = sim_data[sim]\n",
       "   192                                                       if is_proportion is not None:\n",
       "   193                                                           if is_proportion:\n",
       "   194                                                               n = int(round(split[sim] * x.shape[0]))\n",
       "   195                                                           else:\n",
       "   196                                                               n = int(round(split[sim] / 100.0 * x.shape[0]))\n",
       "   197                                                       else:\n",
       "   198                                                           n = min(split[sim], x.shape[0])\n",
       "   199                                                       idx = rng.permutation(x.shape[0])[:n]\n",
       "   200                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   201         1       1116.0   1116.0      0.0      elif split is not None:\n",
       "   202                                                   raise ValueError(\"split must be 'all', 'equal', an integer, a dictionary, or None\")\n",
       "   203                                           \n",
       "   204                                               # Concatenate the data\n",
       "   205         4       7022.0   1755.5      0.0      x_arrays = [v[0] for v in sim_data.values() if v is not None]\n",
       "   206         4       5456.0   1364.0      0.0      y_arrays = [v[1] for v in sim_data.values() if v is not None]\n",
       "   207         1 8762497760.0    9e+09     19.9      x_data = np.concatenate(x_arrays, axis=0) if x_arrays else np.array([])\n",
       "   208         1   25952456.0    3e+07      0.1      y_data = np.concatenate(y_arrays, axis=0) if y_arrays else np.array([])\n",
       "   209                                           \n",
       "   210                                               # Concatenate the data\n",
       "   211         4      11080.0   2770.0      0.0      x_arrays = [v[0] for v in sim_data.values() if v is not None]\n",
       "   212         4       2731.0    682.8      0.0      y_arrays = [v[1] for v in sim_data.values() if v is not None]\n",
       "   213         1 3831490090.0    4e+09      8.7      x_data = np.concatenate(x_arrays, axis=0) if x_arrays else np.array([])\n",
       "   214         1   26324197.0    3e+07      0.1      y_data = np.concatenate(y_arrays, axis=0) if y_arrays else np.array([])\n",
       "   215                                               \n",
       "   216                                               if noise > 0:\n",
       "   217         1        2e+10    2e+10     48.8          if verbose:\n",
       "   218                                                       print(f'Adding noise with amplitude of {noise}K to the data')\n",
       "   219         1     319111.0 319111.0      0.0          x_data += np.random.randn(*x_data.shape) * noise\n",
       "   220                                               \n",
       "   221                                               print('Total number of spectra:', x_data.shape[0])\n",
       "   222                                           \n",
       "   223                                               # Remove lines of sight with NaNs or all zeros\n",
       "   224                                               # nanIndices = np.isnan(spectra).any(axis=1) | np.isnan(fcnm) | np.isnan(funm) | np.isnan(fwnm) | np.isnan(rhi) | np.all(spectra == 0, axis=1)\n",
       "   225                                               # spectra = spectra[~nanIndices]\n",
       "   226                                               # fcnm = fcnm[~nanIndices]\n",
       "   227                                               # funm = funm[~nanIndices]\n",
       "   228                                               # fwnm = fwnm[~nanIndices]\n",
       "   229                                               # rhi = rhi[~nanIndices]\n",
       "   230                                               # los_removed = np.sum(nanIndices)\n",
       "   231                                                   \n",
       "   232                                               # print(f'Removed {los_removed} lines of sight with NaNs')\n",
       "   233         1       6064.0   6064.0      0.0      \n",
       "   234         1       1361.0   1361.0      0.0      # Split the data into training, validation, and testing sets\n",
       "   235         1        416.0    416.0      0.0      train_pct = round((1 - test_size - val_size) * 100)\n",
       "   236                                               val_pct = round(val_size * 100)\n",
       "   237         1       2341.0   2341.0      0.0      test_pct = round(test_size * 100)\n",
       "   238         1        938.0    938.0      0.0      \n",
       "   239         1       1493.0   1493.0      0.0      n_train_samples = round(x_data.shape[0] * (1 - test_size - val_size))\n",
       "   240                                               n_val_samples = round(x_data.shape[0] * val_size)\n",
       "   241         1        911.0    911.0      0.0      n_test_samples = round(x_data.shape[0] * test_size)\n",
       "   242                                               \n",
       "   243                                               if n_train_samples < 1 or n_val_samples < 1 or n_test_samples < 1:\n",
       "   244         1      16649.0  16649.0      0.0          raise ValueError(\"The defined splits result in one or more datasets with less than one sample.\")\n",
       "   245         1 2018572001.0    2e+09      4.6      \n",
       "   246         1  784606006.0    8e+08      1.8      print(f'Splitting data into {train_pct}% train, {val_pct}% validation, and {test_pct}% test sets.')\n",
       "   247                                               X_train, X_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size=n_val_samples+n_test_samples, random_state=random_state)\n",
       "   248         1  809769213.0    8e+08      1.8      X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=n_test_samples / (n_val_samples+n_test_samples), random_state=random_state)\n",
       "   249         1  268151527.0    3e+08      0.6      \n",
       "   250         1  268480392.0    3e+08      0.6      train_dataset = TensorDataset(Tensor(X_train), Tensor(y_train))\n",
       "   251                                               val_dataset = TensorDataset(Tensor(X_val), Tensor(y_val))\n",
       "   252         1     265099.0 265099.0      0.0      test_dataset = TensorDataset(Tensor(X_test), Tensor(y_test))\n",
       "   253         1      52583.0  52583.0      0.0      \n",
       "   254         1      43568.0  43568.0      0.0      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
       "   255                                               val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
       "   256         1        845.0    845.0      0.0      test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
       "   257                                               \n",
       "   258                                               if show_example:\n",
       "   259                                                   import matplotlib.pyplot as plt\n",
       "   260                                                   random_indices = np.random.choice(spectra.shape[0], size=5, replace=False)\n",
       "   261                                                   plt.figure(figsize=(10, 6))\n",
       "   262                                                   for i in random_indices:\n",
       "   263                                                       if x_values == 'emission':\n",
       "   264                                                           plt.plot(spectra[i].T)\n",
       "   265                                                       elif x_values == 'absorption':\n",
       "   266                                                           plt.plot(np.exp(-spectra[i]).T)\n",
       "   267                                                   plt.xlabel('Channel')\n",
       "   268                                                   if x_values == 'emission':\n",
       "   269                                                       plt.ylabel(r'$T_B$ [K]')\n",
       "   270                                                   elif x_values == 'absorption':\n",
       "   271                                                       plt.ylabel(r'$e^{-\\tau}$')\n",
       "   272                                                   plt.title('Example Spectra')\n",
       "   273         1       9478.0   9478.0      0.0          plt.show()\n",
       "   274                                               \n",
       "   275                                               return train_loader, val_loader, test_loader"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f bayeshi.load_data -s bayeshi.load_data(seta_sim='comp', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20cde27-8252-4dc5-8155-7f10e38dfb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818f39b7-1a3b-4849-881c-6407844209ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TIGRESS simulation cube 290 (1/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 300 (2/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 310 (3/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 320 (4/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 330 (5/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 340 (6/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 350 (7/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 360 (8/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 370 (9/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 380 (10/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 390 (11/11) with x_values=emission and y_values=absorption\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 314.523 s\n",
       "File: /home/120/em8117/BayesHI/bayeshi/data_loaders.py\n",
       "Function: load_tigress_data at line 277\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   277                                           def load_tigress_data(data_path, sim_number='all', x_values='emission', y_values='fractions', verbose=False):\n",
       "   278         1       2566.0   2566.0      0.0      if sim_number == 'all':\n",
       "   279         1      25861.0  25861.0      0.0          sim_number = np.arange(290, 391, 10)\n",
       "   280                                               elif type(sim_number) is list:\n",
       "   281                                                   sim_number = [int(s) for s in sim_number]\n",
       "   282                                               else:\n",
       "   283                                                   try:\n",
       "   284                                                       sim_number = [int(sim_number)]\n",
       "   285                                                   except:\n",
       "   286                                                       raise ValueError(\"sim_number should be 'all', a list of integers, or a single integer.\")\n",
       "   287                                                   \n",
       "   288         1       1126.0   1126.0      0.0      if x_values not in ['emission', 'absorption']:\n",
       "   289                                                   raise ValueError(\"x_values must be either 'emission' or 'absorption'\")\n",
       "   290         1        937.0    937.0      0.0      if y_values not in ['fractions', 'absorption', 'emission']:\n",
       "   291                                                   raise ValueError(\"y_values must be either 'fractions', 'absorption', or 'emission'\")\n",
       "   292                                           \n",
       "   293                                               # x_data = np.array([])\n",
       "   294                                               # y_data = np.array([])\n",
       "   295                                               # Preallocate arrays to hold data\n",
       "   296                                               # All TIGRESS cubes are 256 x 256 x 512 (v, z, x)\n",
       "   297         1       2410.0   2410.0      0.0      total_spectra = len(sim_number) * 256 * 512\n",
       "   298         1      30167.0  30167.0      0.0      x_data = np.empty((total_spectra, 256))  # 256 channels\n",
       "   299         1       1404.0   1404.0      0.0      if y_values == 'fractions':\n",
       "   300                                                   y_data = np.empty((total_spectra, 4))\n",
       "   301                                               else:\n",
       "   302         1      20110.0  20110.0      0.0          y_data = np.empty((total_spectra, 256))\n",
       "   303                                               \n",
       "   304        12      96379.0   8031.6      0.0      for i, sim in enumerate(sim_number):\n",
       "   305        11      12571.0   1142.8      0.0          if verbose:\n",
       "   306        11    1618327.0 147120.6      0.0              print(f'Loading TIGRESS simulation cube {sim} ({i+1}/{len(sim_number)}) with x_values={x_values} and y_values={y_values}')\n",
       "   307        11       7608.0    691.6      0.0          if x_values == 'emission':\n",
       "   308        11  120650939.0    1e+07      0.0              spectra = fits.getdata(data_path + f'{sim}_Tb_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   309                                                   elif x_values == 'absorption':\n",
       "   310                                                       spectra = fits.getdata(data_path + f'{sim}_Tau_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   311                                                   else:\n",
       "   312                                                       raise ValueError(\"x_values must be either 'emission' or 'absorption'\")\n",
       "   313                                                           \n",
       "   314                                                   # Change from (v, z, x) to (x*z, v)\n",
       "   315        11     506403.0  46036.6      0.0          spectra = np.moveaxis(spectra, 0, -1)\n",
       "   316        11      36976.0   3361.5      0.0          spectra = spectra.reshape(-1, spectra.shape[-1])\n",
       "   317                                                   \n",
       "   318                                                   # Add the spectra to x_data\n",
       "   319        11       9878.0    898.0      0.0          start_idx = (i * 256 * 512)\n",
       "   320        11       7846.0    713.3      0.0          end_idx = start_idx + spectra.shape[0]\n",
       "   321        11 2256956503.0    2e+08      0.7          x_data[start_idx:end_idx, :] = spectra\n",
       "   322                                                   \n",
       "   323        11      38816.0   3528.7      0.0          if y_values == 'fractions':\n",
       "   324                                                       fcnm = fits.getdata(data_path + f'{sim}_fcnm_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   325                                                       funm = fits.getdata(data_path + f'{sim}_funm_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   326                                                       fwnm = fits.getdata(data_path + f'{sim}_fwnm_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   327                                                       rhi = fits.getdata(data_path + f'{sim}_rhi_FINAL.fits')[3584//2-128:3584//2+128, :]\n",
       "   328                                                       \n",
       "   329                                                       # Stack from (z, x) to (z, x, 4)\n",
       "   330                                                       fractions = np.stack((fcnm, funm, fwnm, rhi), axis=2)\n",
       "   331                                                       # Change from (z, x, 4) to (x*z, 4)\n",
       "   332                                                       fractions = fractions.reshape(-1, 4)\n",
       "   333                                                       y_data_temp = fractions\n",
       "   334                                                       \n",
       "   335        11       7790.0    708.2      0.0          elif y_values == 'absorption':\n",
       "   336        11  158432942.0    1e+07      0.1              absorption = fits.getdata(data_path + f'{sim}_Tau_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   337                                                       # Change from (v, z, x) to (x*z, v)\n",
       "   338        11     410113.0  37283.0      0.0              absorption = np.moveaxis(absorption, 0, -1)\n",
       "   339        11  122041484.0    1e+07      0.0              y_data_temp = absorption.reshape(-1, absorption.shape[-1])\n",
       "   340                                                   elif y_values == 'emission':\n",
       "   341                                                       emission = fits.getdata(data_path + f'{sim}_Tb_FINAL.fits')[:, 3584//2-128:3584//2+128, :]\n",
       "   342                                                       # Change from (v, z, x) to (x*z, v)\n",
       "   343                                                       emission = np.moveaxis(emission, 0, -1)\n",
       "   344                                                       y_data_temp = emission.reshape(-1, emission.shape[-1])\n",
       "   345                                                   else:\n",
       "   346                                                       raise ValueError(\"y_values must be either 'fractions', 'absorption', or 'emission'\")\n",
       "   347                                               \n",
       "   348                                                   # Add the y_data to y_data\n",
       "   349        11        3e+11    3e+10     99.2          y_data[start_idx:end_idx, :] = y_data_temp\n",
       "   350                                                       \n",
       "   351         1       5658.0   5658.0      0.0      return x_data, y_data"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f bayeshi.data_loaders.load_tigress_data -s bayeshi.data_loaders.load_tigress_data(data_path='/scratch/mk27/em8117/TIGRESS/', verbose=True, y_values='absorption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564b0e4-234a-49ee-936d-d264c9756178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420458e-7687-4dd8-9066-d6f1500a0436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7c4bd4-c862-44ab-baef-cd4f0a72f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TIGRESS simulation cube 290 (1/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 300 (2/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 310 (3/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 320 (4/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 330 (5/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 340 (6/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 350 (7/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 360 (8/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 370 (9/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 380 (10/11) with x_values=emission and y_values=absorption\n",
      "Loading TIGRESS simulation cube 390 (11/11) with x_values=emission and y_values=absorption\n",
      "Loading Saury data with x_values=emission and y_values=absorption\n",
      "Loading Seta simulation comp with x_values=emission and y_values=absorption\n",
      "Adding noise with amplitude of 0.5K to the data\n",
      "Total number of spectra: 2752512\n",
      "Splitting data into 60% train, 20% validation, and 20% test sets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 52.3257 s\n",
       "File: /home/120/em8117/BayesHI/bayeshi/data_loaders.py\n",
       "Function: load_data at line 66\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    66                                           def load_data(data_path='/scratch/mk27/em8117/', x_values='emission', y_values='fractions',\n",
       "    67                                                         dataset='all', tigress_sim='all', seta_sim='both', split=None,\n",
       "    68                                                         batch_size=32, num_workers=4, noise=0.5, test_size=0.2, val_size=0.2,\n",
       "    69                                                         random_state=42, show_example=False, verbose=False):\n",
       "    70                                               # If you need to save space in memory and are loading a subset of the data, I recommend not using all the TIGRESS cubes.\n",
       "    71                                               # Instead, select only a few of them as loading all 11 cubes just to subsample at the end is not efficient.\n",
       "    72                                           \n",
       "    73                                               # Validate x_values and y_values\n",
       "    74         1       1053.0   1053.0      0.0      if x_values not in ['emission', 'absorption']:\n",
       "    75                                                   raise ValueError(\"x_values must be either 'emission' or 'absorption'\")\n",
       "    76         1        423.0    423.0      0.0      if y_values not in ['fractions', 'absorption', 'emission']:\n",
       "    77                                                   raise ValueError(\"y_values must be either 'fractions', 'absorption', or 'emission'\")\n",
       "    78                                           \n",
       "    79                                               # Create empty arrays to be overwritten as needed\n",
       "    80         1      10441.0  10441.0      0.0      tigress_x, tigress_y = np.array([]), np.array([])\n",
       "    81         1       1383.0   1383.0      0.0      saury_x, saury_y = np.array([]), np.array([])\n",
       "    82         1       1217.0   1217.0      0.0      seta_x, seta_y = np.array([]), np.array([])\n",
       "    83                                           \n",
       "    84                                               # Load data based on dataset parameter\n",
       "    85         1       2973.0   2973.0      0.0      if dataset == 'all':\n",
       "    86         1 4836523297.0    5e+09      9.2          tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "    87         1    8575857.0    9e+06      0.0          saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "    88         1    9258288.0    9e+06      0.0          seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "    89                                               elif isinstance(dataset, list):\n",
       "    90                                                   for sim in dataset:\n",
       "    91                                                       if sim.startswith('tigress'):\n",
       "    92                                                           tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "    93                                                       elif sim.startswith('saury'):\n",
       "    94                                                           saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "    95                                                       elif sim.startswith('seta'):\n",
       "    96                                                           seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "    97                                                       else:\n",
       "    98                                                           raise ValueError(f\"Unknown dataset type: {sim}\")\n",
       "    99                                               elif isinstance(dataset, str):\n",
       "   100                                                   if dataset.startswith('tigress'):\n",
       "   101                                                       tigress_x, tigress_y = load_tigress_data(data_path + 'TIGRESS/', tigress_sim, x_values, y_values, verbose=verbose)\n",
       "   102                                                   elif dataset.startswith('saury'):\n",
       "   103                                                       saury_x, saury_y = load_saury_data(data_path + 'Saury/', x_values, y_values, verbose=verbose)\n",
       "   104                                                   elif dataset.startswith('seta'):\n",
       "   105                                                       seta_x, seta_y = load_seta_data(data_path + 'Seta/', seta_sim, x_values, y_values, verbose=verbose)\n",
       "   106                                                   else:\n",
       "   107                                                       raise ValueError(f\"Unknown dataset type: {dataset}\")\n",
       "   108                                               else:\n",
       "   109                                                   raise ValueError(\"dataset must be 'all', a list of dataset names, or a single dataset name string\")\n",
       "   110                                           \n",
       "   111                                               # Collect non-empty simulations\n",
       "   112         1       3085.0   3085.0      0.0      sim_data = {\n",
       "   113         1        979.0    979.0      0.0          'tigress': (tigress_x, tigress_y) if tigress_x.size > 0 else None,\n",
       "   114         1        702.0    702.0      0.0          'saury': (saury_x, saury_y) if saury_x.size > 0 else None,\n",
       "   115         1        628.0    628.0      0.0          'seta': (seta_x, seta_y) if seta_x.size > 0 else None\n",
       "   116                                               }\n",
       "   117         4       3987.0    996.8      0.0      active_sims = [k for k, v in sim_data.items() if v is not None]\n",
       "   118         1        370.0    370.0      0.0      if not active_sims:\n",
       "   119                                                   raise ValueError(\"No valid simulations loaded\")\n",
       "   120                                           \n",
       "   121                                               # Process split argument\n",
       "   122         1        541.0    541.0      0.0      if split == 'all':\n",
       "   123                                                   # Use all available data\n",
       "   124                                                   pass\n",
       "   125         1       2135.0   2135.0      0.0      elif split == 'equal':\n",
       "   126                                                   # Use equal number of samples from each loaded simulation\n",
       "   127                                                   n_per_sim = min(v[0].shape[0] for v in sim_data.values() if v is not None)\n",
       "   128                                                   rng = np.random.RandomState(random_state)\n",
       "   129                                                   for sim in active_sims:\n",
       "   130                                                       x, y = sim_data[sim]\n",
       "   131                                                       idx = rng.permutation(x.shape[0])[:n_per_sim]\n",
       "   132                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   133         1       1943.0   1943.0      0.0      elif isinstance(split, int):\n",
       "   134                                                   # Split as equally as possible across simulations\n",
       "   135                                                   n_total = split\n",
       "   136                                                   n_sims = len(active_sims)\n",
       "   137                                                   n_per_sim = n_total // n_sims\n",
       "   138                                                   remainder = n_total % n_sims\n",
       "   139                                                   print(f\"Splitting {n_total} samples across {n_sims} simulations: {n_per_sim} per simulation, with {remainder} extra samples distributed\")\n",
       "   140                                                   rng = np.random.RandomState(random_state)\n",
       "   141                                                   for i, sim in enumerate(active_sims):\n",
       "   142                                                       x, y = sim_data[sim]\n",
       "   143                                                       n = n_per_sim + (1 if i < remainder else 0)\n",
       "   144                                                       if n > x.shape[0]:\n",
       "   145                                                           raise ValueError(f\"Not enough samples in {sim} for split={split}\")\n",
       "   146                                                       idx = rng.permutation(x.shape[0])[:n]\n",
       "   147                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   148         1        955.0    955.0      0.0      elif isinstance(split, dict):\n",
       "   149                                                   # Dictionary split (proportion, percent, or absolute)\n",
       "   150                                                   required_keys = {'tigress', 'saury', 'seta'}\n",
       "   151                                                   if not required_keys.issubset(split.keys()):\n",
       "   152                                                       raise ValueError(\"split dictionary must contain 'tigress', 'saury', and 'seta' keys\")\n",
       "   153                                                   for key in required_keys:\n",
       "   154                                                       if not isinstance(split[key], (int, float)):\n",
       "   155                                                           raise ValueError(f\"split['{key}'] must be a number\")\n",
       "   156                                                   sum_split = sum(split.values())\n",
       "   157                                                   # Check if numbers are proportions, percentages, or absolute numbers\n",
       "   158                                                   if all(0 <= v <= 1 for v in split.values()):\n",
       "   159                                                       if np.isclose(sum_split, 1.0):\n",
       "   160                                                           is_proportion = True  # proportions\n",
       "   161                                                           # Print the splits as percentages for each simulation\n",
       "   162                                                           print(\"Splitting data as proportions:\")\n",
       "   163                                                           for sim in active_sims:\n",
       "   164                                                               print(f\"{sim}: {split[sim] * 100:.2f}%\")\n",
       "   165                                                       else:\n",
       "   166                                                           raise ValueError(\"if using proportions for data splitting, the sum of the split values must be 1.0\")\n",
       "   167                                                   elif all(0 <= v <= 100 for v in split.values()):\n",
       "   168                                                       if np.isclose(sum_split, 100.0):\n",
       "   169                                                           is_proportion = False  # percentages\n",
       "   170                                                           # Print the splits as percentages for each simulation\n",
       "   171                                                           print(\"Splitting data as percentages:\")\n",
       "   172                                                           for sim in active_sims:\n",
       "   173                                                               print(f\"{sim}: {split[sim]:.2f}%\")\n",
       "   174                                                       else:\n",
       "   175                                                           # Treat as absolute numbers if not close to 100\n",
       "   176                                                           warnings.warn(\"Splitting data as absolute numbers:\", UserWarning)\n",
       "   177                                                           for sim in active_sims:\n",
       "   178                                                               print(f\"{sim}: {split[sim]} spectra\")\n",
       "   179                                                           is_proportion = None  # absolute numbers\n",
       "   180                                                   elif all(isinstance(v, int) for v in split.values()):\n",
       "   181                                                       print(\"Splitting data as absolute numbers:\")\n",
       "   182                                                       for sim in active_sims:\n",
       "   183                                                           print(f\"{sim}: {split[sim]} spectra\")\n",
       "   184                                                       is_proportion = None  # absolute numbers\n",
       "   185                                                       if any(v < 0 for v in split.values()):\n",
       "   186                                                           raise ValueError(\"if using absolute numbers for data splitting, all values must be non-negative\")\n",
       "   187                                                   else:\n",
       "   188                                                       raise ValueError(\"split values must be either proportions (0-1), percentages (0-100), or absolute numbers (non-negative integers)\")\n",
       "   189                                                   rng = np.random.RandomState(random_state)\n",
       "   190                                                   for sim in active_sims:\n",
       "   191                                                       x, y = sim_data[sim]\n",
       "   192                                                       if is_proportion is not None:\n",
       "   193                                                           if is_proportion:\n",
       "   194                                                               n = int(round(split[sim] * x.shape[0]))\n",
       "   195                                                           else:\n",
       "   196                                                               n = int(round(split[sim] / 100.0 * x.shape[0]))\n",
       "   197                                                       else:\n",
       "   198                                                           n = min(split[sim], x.shape[0])\n",
       "   199                                                       idx = rng.permutation(x.shape[0])[:n]\n",
       "   200                                                       sim_data[sim] = (x[idx], y[idx])\n",
       "   201         1        429.0    429.0      0.0      elif split is not None:\n",
       "   202                                                   raise ValueError(\"split must be 'all', 'equal', an integer, a dictionary, or None\")\n",
       "   203                                           \n",
       "   204                                               # Concatenate the data\n",
       "   205         4       3509.0    877.2      0.0      x_arrays = [v[0] for v in sim_data.values() if v is not None]\n",
       "   206         4       1853.0    463.2      0.0      y_arrays = [v[1] for v in sim_data.values() if v is not None]\n",
       "   207         1 4071272834.0    4e+09      7.8      # Pre-allocate arrays for speed if possible\n",
       "   208         1 4009088698.0    4e+09      7.7      total_samples = sum(arr.shape[0] for arr in x_arrays)\n",
       "   209                                               x_shape = x_arrays[0].shape[1:] if x_arrays else ()\n",
       "   210                                               y_shape = y_arrays[0].shape[1:] if y_arrays else ()\n",
       "   211         4      11679.0   2919.8      0.0  \n",
       "   212         4       2647.0    661.8      0.0      x_data = np.empty((total_samples, *x_shape), dtype=x_arrays[0].dtype) if x_arrays else np.array([])\n",
       "   213         1 3958850562.0    4e+09      7.6      y_data = np.empty((total_samples, *y_shape), dtype=y_arrays[0].dtype) if y_arrays else np.array([])\n",
       "   214         1 3635698059.0    4e+09      6.9  \n",
       "   215                                               idx = 0\n",
       "   216         1       3184.0   3184.0      0.0      for x_arr, y_arr in zip(x_arrays, y_arrays):\n",
       "   217         1        444.0    444.0      0.0          n = x_arr.shape[0]\n",
       "   218         1     131709.0 131709.0      0.0          x_data[idx:idx+n] = x_arr\n",
       "   219         1        2e+10    2e+10     42.4          y_data[idx:idx+n] = y_arr\n",
       "   220                                                   idx += n\n",
       "   221         1     120200.0 120200.0      0.0  \n",
       "   222                                               # # Concatenate the data\n",
       "   223                                               # x_arrays = [v[0] for v in sim_data.values() if v is not None]\n",
       "   224                                               # y_arrays = [v[1] for v in sim_data.values() if v is not None]\n",
       "   225                                               # x_data = np.concatenate(x_arrays, axis=0) if x_arrays else np.array([])\n",
       "   226                                               # y_data = np.concatenate(y_arrays, axis=0) if y_arrays else np.array([])\n",
       "   227                                               \n",
       "   228                                               if noise > 0:\n",
       "   229                                                   if verbose:\n",
       "   230                                                       print(f'Adding noise with amplitude of {noise}K to the data')\n",
       "   231                                                   x_data += np.random.randn(*x_data.shape) * noise\n",
       "   232                                               \n",
       "   233                                               print('Total number of spectra:', x_data.shape[0])\n",
       "   234                                           \n",
       "   235         1       3926.0   3926.0      0.0      # Remove lines of sight with NaNs or all zeros\n",
       "   236         1        627.0    627.0      0.0      # nanIndices = np.isnan(spectra).any(axis=1) | np.isnan(fcnm) | np.isnan(funm) | np.isnan(fwnm) | np.isnan(rhi) | np.all(spectra == 0, axis=1)\n",
       "   237         1        422.0    422.0      0.0      # spectra = spectra[~nanIndices]\n",
       "   238                                               # fcnm = fcnm[~nanIndices]\n",
       "   239         1       1210.0   1210.0      0.0      # funm = funm[~nanIndices]\n",
       "   240         1        672.0    672.0      0.0      # fwnm = fwnm[~nanIndices]\n",
       "   241         1        651.0    651.0      0.0      # rhi = rhi[~nanIndices]\n",
       "   242                                               # los_removed = np.sum(nanIndices)\n",
       "   243         1       1023.0   1023.0      0.0          \n",
       "   244                                               # print(f'Removed {los_removed} lines of sight with NaNs')\n",
       "   245                                               \n",
       "   246         1      15491.0  15491.0      0.0      # Split the data into training, validation, and testing sets\n",
       "   247         1 4514707138.0    5e+09      8.6      train_pct = round((1 - test_size - val_size) * 100)\n",
       "   248         1 2107179521.0    2e+09      4.0      val_pct = round(val_size * 100)\n",
       "   249                                               test_pct = round(test_size * 100)\n",
       "   250         1 1772179917.0    2e+09      3.4      \n",
       "   251         1  604731280.0    6e+08      1.2      n_train_samples = round(x_data.shape[0] * (1 - test_size - val_size))\n",
       "   252         1  603723944.0    6e+08      1.2      n_val_samples = round(x_data.shape[0] * val_size)\n",
       "   253                                               n_test_samples = round(x_data.shape[0] * test_size)\n",
       "   254         1     455937.0 455937.0      0.0      \n",
       "   255         1      60301.0  60301.0      0.0      if n_train_samples < 1 or n_val_samples < 1 or n_test_samples < 1:\n",
       "   256         1      45360.0  45360.0      0.0          raise ValueError(\"The defined splits result in one or more datasets with less than one sample.\")\n",
       "   257                                               \n",
       "   258         1        830.0    830.0      0.0      print(f'Splitting data into {train_pct}% train, {val_pct}% validation, and {test_pct}% test sets.')\n",
       "   259                                               X_train, X_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size=n_val_samples+n_test_samples, random_state=random_state)\n",
       "   260                                               X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=n_test_samples / (n_val_samples+n_test_samples), random_state=random_state)\n",
       "   261                                               \n",
       "   262                                               train_dataset = TensorDataset(Tensor(X_train), Tensor(y_train))\n",
       "   263                                               val_dataset = TensorDataset(Tensor(X_val), Tensor(y_val))\n",
       "   264                                               test_dataset = TensorDataset(Tensor(X_test), Tensor(y_test))\n",
       "   265                                               \n",
       "   266                                               train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
       "   267                                               val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
       "   268                                               test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
       "   269                                               \n",
       "   270                                               if show_example:\n",
       "   271                                                   import matplotlib.pyplot as plt\n",
       "   272                                                   random_indices = np.random.choice(spectra.shape[0], size=5, replace=False)\n",
       "   273                                                   plt.figure(figsize=(10, 6))\n",
       "   274                                                   for i in random_indices:\n",
       "   275         1      10445.0  10445.0      0.0              if x_values == 'emission':\n",
       "   276                                                           plt.plot(spectra[i].T)\n",
       "   277                                                       elif x_values == 'absorption':\n",
       "   278                                                           plt.plot(np.exp(-spectra[i]).T)\n",
       "   279                                                   plt.xlabel('Channel')\n",
       "   280                                                   if x_values == 'emission':\n",
       "   281                                                       plt.ylabel(r'$T_B$ [K]')\n",
       "   282                                                   elif x_values == 'absorption':\n",
       "   283                                                       plt.ylabel(r'$e^{-\\tau}$')\n",
       "   284                                                   plt.title('Example Spectra')\n",
       "   285                                                   plt.show()\n",
       "   286                                               \n",
       "   287                                               return train_loader, val_loader, test_loader"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f bayeshi.load_data -s bayeshi.load_data(y_values='absorption', seta_sim='comp', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03639039-27eb-4326-aab4-07c6126af763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb015f88-54d8-42ff-95d6-671bcea8d941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17048e-4944-44e7-a9ed-e076e636d737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a53f9d-e037-4f8b-9df4-45b87412308c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff9ce6-2c92-48c0-ba11-f762d56a675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 3000 samples across 1 simulations: 3000 per simulation, with 0 extra samples distributed\n"
     ]
    }
   ],
   "source": [
    "with Profiler(interval=0.01) as profiler:\n",
    "    train_loader, val_loader, test_loader = bayeshi.load_data(dataset='seta', split=1000, seta_sim='comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f6946-7bcf-4c05-99a9-f1661e2bd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Profiler(interval=0.01) as profiler:\n",
    "    train_loader, val_loader, test_loader = bayeshi.load_data(dataset='tigress', split=1000, tigress_sim=290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a886c4c-d508-456e-85fb-f7fafb0a09db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 30000 samples across 3 simulations: 10000 per simulation, with 0 extra samples distributed\n",
      "Total number of spectra: 30000\n",
      "Splitting data into 60% train, 20% validation, and 20% test sets.\n",
      "Training Model\n",
      "Initial learning rate: [0.001]\n",
      "Epoch [1/50], Train Loss: 0.2557, Validation Loss: 0.2549, took 7.41s\n",
      "Epoch [2/50], Train Loss: 0.2555, Validation Loss: 0.2546, took 7.26s\n",
      "Epoch [3/50], Train Loss: 0.2548, Validation Loss: 0.2535, took 7.25s\n",
      "Epoch [4/50], Train Loss: 0.2550, Validation Loss: 0.2555, took 7.27s\n",
      "Epoch [5/50], Train Loss: 0.2552, Validation Loss: 0.2546, took 7.29s\n",
      "Epoch [6/50], Train Loss: 0.2551, Validation Loss: 0.2548, took 7.26s\n",
      "Epoch [7/50], Train Loss: 0.2552, Validation Loss: 0.2539, took 7.20s\n",
      "Epoch [8/50], Train Loss: 0.2552, Validation Loss: 0.2536, took 7.43s\n",
      "Epoch [9/50], Train Loss: 0.2555, Validation Loss: 0.2547, took 7.01s\n",
      "Epoch [10/50], Train Loss: 0.2549, Validation Loss: 0.2536, took 6.36s\n",
      "Epoch [11/50], Train Loss: 0.2554, Validation Loss: 0.2538, took 6.44s\n",
      "Epoch [12/50], Train Loss: 0.2544, Validation Loss: 0.2551, took 6.35s\n",
      "Epoch [13/50], Train Loss: 0.2553, Validation Loss: 0.2539, took 6.32s\n",
      "Epoch [14/50], Train Loss: 0.2546, Validation Loss: 0.2532, took 6.49s\n",
      "Epoch [15/50], Train Loss: 0.2541, Validation Loss: 0.2541, took 6.33s\n",
      "Epoch [16/50], Train Loss: 0.2539, Validation Loss: 0.2532, took 6.29s\n",
      "Epoch [17/50], Train Loss: 0.2541, Validation Loss: 0.2534, took 6.41s\n",
      "Epoch [18/50], Train Loss: 0.2537, Validation Loss: 0.2524, took 6.39s\n",
      "Epoch [19/50], Train Loss: 0.2535, Validation Loss: 0.2527, took 6.38s\n",
      "Epoch [20/50], Train Loss: 0.2537, Validation Loss: 0.2526, took 6.49s\n",
      "Epoch [21/50], Train Loss: 0.2534, Validation Loss: 0.2525, took 6.26s\n",
      "Epoch [22/50], Train Loss: 0.2536, Validation Loss: 0.2541, took 6.39s\n",
      "Epoch [23/50], Train Loss: 0.2538, Validation Loss: 0.2526, took 7.06s\n",
      "Epoch [24/50], Train Loss: 0.2537, Validation Loss: 0.2529, took 7.29s\n",
      "Epoch [25/50], Train Loss: 0.2536, Validation Loss: 0.2529, took 7.33s\n",
      "Epoch [26/50], Train Loss: 0.2535, Validation Loss: 0.2524, took 7.30s\n",
      "Epoch [27/50], Train Loss: 0.2533, Validation Loss: 0.2523, took 7.24s\n",
      "Epoch [28/50], Train Loss: 0.2538, Validation Loss: 0.2530, took 7.25s\n",
      "Epoch [29/50], Train Loss: 0.2537, Validation Loss: 0.2532, took 7.25s\n",
      "Epoch [30/50], Train Loss: 0.2535, Validation Loss: 0.2523, took 7.27s\n",
      "Epoch [31/50], Train Loss: 0.2538, Validation Loss: 0.2527, took 7.31s\n",
      "Epoch [32/50], Train Loss: 0.2534, Validation Loss: 0.2531, took 7.28s\n",
      "Epoch [33/50], Train Loss: 0.2537, Validation Loss: 0.2526, took 7.29s\n",
      "Epoch [34/50], Train Loss: 0.2539, Validation Loss: 0.2528, took 7.36s\n",
      "Epoch [35/50], Train Loss: 0.2538, Validation Loss: 0.2526, took 7.26s\n",
      "Epoch [36/50], Train Loss: 0.2535, Validation Loss: 0.2523, took 7.37s\n",
      "Epoch [37/50], Train Loss: 0.2535, Validation Loss: 0.2524, took 7.33s\n",
      "Epoch [38/50], Train Loss: 0.2533, Validation Loss: 0.2522, took 7.30s\n",
      "Epoch [39/50], Train Loss: 0.2532, Validation Loss: 0.2522, took 7.33s\n",
      "Epoch [40/50], Train Loss: 0.2532, Validation Loss: 0.2526, took 7.33s\n",
      "Epoch [41/50], Train Loss: 0.2532, Validation Loss: 0.2522, took 7.33s\n",
      "Epoch [42/50], Train Loss: 0.2532, Validation Loss: 0.2526, took 7.30s\n",
      "Epoch [43/50], Train Loss: 0.2532, Validation Loss: 0.2525, took 7.36s\n",
      "Epoch [44/50], Train Loss: 0.2531, Validation Loss: 0.2523, took 7.45s\n",
      "Epoch [45/50], Train Loss: 0.2531, Validation Loss: 0.2522, took 7.58s\n",
      "Epoch [46/50], Train Loss: 0.2530, Validation Loss: 0.2520, took 7.35s\n",
      "Epoch [47/50], Train Loss: 0.2529, Validation Loss: 0.2519, took 7.43s\n",
      "Epoch [48/50], Train Loss: 0.2528, Validation Loss: 0.2519, took 7.36s\n",
      "Epoch [49/50], Train Loss: 0.2528, Validation Loss: 0.2519, took 7.93s\n",
      "Epoch [50/50], Train Loss: 0.2526, Validation Loss: 0.2515, took 7.67s\n"
     ]
    }
   ],
   "source": [
    "with Profiler(interval=0.01) as profiler:\n",
    "    model = bayeshi.load_model('LSTMSequenceToSequence')\n",
    "    train_loader, val_loader, test_loader = bayeshi.load_data(y_values='absorption', split=30000, seta_sim='comp')\n",
    "    train_errors, val_errors, *_ = model.fit(train_loader, val_loader, None, nEpochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034c8d41-24b2-47c7-9e1e-6e2d7a8f4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 21:33:53  Samples:  35141\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 364.950   CPU time: 339.547\n",
      "/   _/                      v5.0.2\n",
      "\n",
      "Profile at /jobfs/144302821.gadi-pbs/ipykernel_2208506/931054199.py:1\n",
      "\n",
      "364.948 <module>  ../ipykernel_2208506/931054199.py:1\n",
      "├─ 356.845 LSTMSequenceToSequence.fit  bayeshi/models.py:1254\n",
      "│  ├─ 98.244 Tensor.to  <built-in>\n",
      "│  ├─ 67.977 LSTMSequenceToSequence.evaluate  bayeshi/models.py:1313\n",
      "│  │  ├─ 29.927 Tensor.to  <built-in>\n",
      "│  │  ├─ 15.145 Tensor.item  <built-in>\n",
      "│  │  ├─ 13.572 _MultiProcessingDataLoaderIter.__next__  torch/utils/data/dataloader.py:703\n",
      "│  │  │  └─ 13.020 _MultiProcessingDataLoaderIter._next_data  torch/utils/data/dataloader.py:1424\n",
      "│  │  │     └─ 11.050 _MultiProcessingDataLoaderIter._get_data  torch/utils/data/dataloader.py:1389\n",
      "│  │  │        └─ 11.031 _MultiProcessingDataLoaderIter._try_get_data  torch/utils/data/dataloader.py:1238\n",
      "│  │  │           └─ 11.022 Queue.get  multiprocessing/queues.py:98\n",
      "│  │  │              └─ 8.926 UntypedStorage.rebuild_storage_fd  torch/multiprocessing/reductions.py:540\n",
      "│  │  │                 └─ 8.435 DupFd.detach  multiprocessing/resource_sharer.py:55\n",
      "│  │  │                       [3 frames hidden]  multiprocessing\n",
      "│  │  ├─ 4.355 LSTMSequenceToSequence._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "│  │  │  └─ 4.336 LSTMSequenceToSequence._call_impl  torch/nn/modules/module.py:1743\n",
      "│  │  │     └─ 3.823 LSTMSequenceToSequence.forward  bayeshi/models.py:1231\n",
      "│  │  └─ 3.783 DataLoader.__iter__  torch/utils/data/dataloader.py:478\n",
      "│  │     └─ 3.783 DataLoader._get_iterator  torch/utils/data/dataloader.py:417\n",
      "│  │        └─ 3.783 _MultiProcessingDataLoaderIter.__init__  torch/utils/data/dataloader.py:1080\n",
      "│  │           └─ 3.736 Process.start  multiprocessing/process.py:110\n",
      "│  │                 [3 frames hidden]  multiprocessing\n",
      "│  ├─ 62.803 Tensor.backward  torch/_tensor.py:570\n",
      "│  │  └─ 62.541 backward  torch/autograd/__init__.py:242\n",
      "│  │     └─ 61.770 _engine_run_backward  torch/autograd/graph.py:814\n",
      "│  │        └─ 61.604 _EngineBase.run_backward  <built-in>\n",
      "│  ├─ 50.068 _MultiProcessingDataLoaderIter.__next__  torch/utils/data/dataloader.py:703\n",
      "│  │  └─ 46.928 _MultiProcessingDataLoaderIter._next_data  torch/utils/data/dataloader.py:1424\n",
      "│  │     └─ 43.912 _MultiProcessingDataLoaderIter._get_data  torch/utils/data/dataloader.py:1389\n",
      "│  │        └─ 43.814 _MultiProcessingDataLoaderIter._try_get_data  torch/utils/data/dataloader.py:1238\n",
      "│  │           └─ 43.775 Queue.get  multiprocessing/queues.py:98\n",
      "│  │              └─ 37.327 UntypedStorage.rebuild_storage_fd  torch/multiprocessing/reductions.py:540\n",
      "│  │                 └─ 34.835 DupFd.detach  multiprocessing/resource_sharer.py:55\n",
      "│  │                       [14 frames hidden]  multiprocessing, <built-in>\n",
      "│  ├─ 30.358 Tensor.item  <built-in>\n",
      "│  ├─ 16.388 LSTMSequenceToSequence._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "│  │  └─ 16.224 LSTMSequenceToSequence._call_impl  torch/nn/modules/module.py:1743\n",
      "│  │     └─ 14.293 LSTMSequenceToSequence.forward  bayeshi/models.py:1231\n",
      "│  │        └─ 13.183 LSTM._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "│  │           └─ 13.033 LSTM._call_impl  torch/nn/modules/module.py:1743\n",
      "│  │              ├─ 8.053 LSTM.forward  torch/nn/modules/rnn.py:1042\n",
      "│  │              │  └─ 5.155 _VariableFunctionsClass.lstm  <built-in>\n",
      "│  │              └─ 4.689 Linear.forward  torch/nn/modules/linear.py:124\n",
      "│  │                 └─ 4.475 linear  <built-in>\n",
      "│  ├─ 16.061 wrapper  torch/optim/optimizer.py:473\n",
      "│  │  └─ 13.409 AdamW._use_grad  torch/optim/optimizer.py:72\n",
      "│  │     └─ 13.041 AdamW.step  torch/optim/adamw.py:207\n",
      "│  │        └─ 10.716 maybe_fallback  torch/optim/optimizer.py:140\n",
      "│  │           └─ 10.636 adamw  torch/optim/adamw.py:810\n",
      "│  │              └─ 9.417 _multi_tensor_adamw  torch/optim/adamw.py:486\n",
      "│  ├─ 4.180 DataLoader.__iter__  torch/utils/data/dataloader.py:478\n",
      "│  │  └─ 4.180 DataLoader._get_iterator  torch/utils/data/dataloader.py:417\n",
      "│  │     └─ 4.180 _MultiProcessingDataLoaderIter.__init__  torch/utils/data/dataloader.py:1080\n",
      "│  │        └─ 3.751 Process.start  multiprocessing/process.py:110\n",
      "│  │              [3 frames hidden]  multiprocessing\n",
      "│  └─ 4.099 [self]  bayeshi/models.py\n",
      "└─ 7.761 load_data  bayeshi/data_loaders.py:65\n",
      "   └─ 6.100 load_tigress_data  bayeshi/data_loaders.py:265\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiler.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cdcff-96ea-46fc-832b-99b2599b652a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
